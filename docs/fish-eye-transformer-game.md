# Fish-Eye Transformer Game

## Game-Design Goals

Provide the neural net(s) with a straightforward representation of both the problem context
and the "state of play".

In this game, the "state of play" includes a complete, executable `Generator` clan, so
we must include an arbitrary tree structure in our representation of the "state of play".

## The Board

Each board position in this game corresponds to a complete, executable `Generator` clan.

The board has three components:
* a set (possibly empty) of grids defining the problem setup
* the grid generated by the current `Generator` clan"
* for each imp of the current `Generator` clan", an "imp view" contains:
    * a "fish-eye imp view" of the clan from the perspective of that imp
    * a path down to that imp in the clan
    * the grid generated by the current `Generator` clan with rendering suppressed 
      outside the sept rooted at that imp 
      
### The Fish-Eye Imp View
  
The fish-eye imp view is created by projecting the rest of the clan through neural net layers.

For the purpose of creating the projection, every possible imp value is mapped to a single one-hot integer,
assigned within a single integer range that covers all imp types. 
This one-hot integer range has a unique value for 
* every `Mix` subclass
* every possible `Int` role value 
  (hence `Int` role values must be constrained to a modest range; 
  we should probably define a new data type for this sort of bounded integer), 
* every possible value of every enum-valued role
* etc.
The one-hot integer range also has a unique value representing "not present" (similar in meaning to `null`).

As is standard for one-hot encoding, the network ends up learning a vector to represent each one-hot value.
But these vectors are learned and applied in a unique framework: the fish-eye imp view.

Every possible role of any imp is also mapped to a single integer, the "row index" for that role.
The imp itself has a row index, which can be thought of as a special `this` role. 
For `List`-valued roles, we treat the list index as a second, finer-grained tier of the role name. So 
each index of a `List`-valued role gets its own row index.

The fish-eye projection is performed in two passes: bottom-up and top-down.

The bottom-up pass assigns a value to each leaf imp and then rolls these up recursively to assign
a value to each sept (i.e. each clan subtree).
* The value of each leaf imp is the vector representation learned for that imp's one-hot value.
* The value of a sept rooted at a given interior imp (the "sept root imp") is computed by 
    rolling up the values of that imp and its roles as follows:
    * We define a two-dimensional "role matrix" to represent the entire set of role values for this imp.
        * The row indices are those assigned to roles as described above.
        * The row corresponding to the special `this` role contains the vector representation learned for
          the sept root imp's one-hot value.
        * Each other row contains the value assigned by this rollup process to the sept at that role,
          or the learned "not present" value if the sept root imp doesn't have that role.
    * We use a chunk of neural net to map the role matrix into a value for this sept.

The top-down pass provides a "context matrix" for each sept root imp. This is simply a two-dimensional 
matrix where row 0 is the value of the sept root imp's immediate parent, row 1 is the value of its
grandparent, and so on, up to some pre-defined maximum nesting depth. 

### The Path to an Imp

The path to an imp in a clan is represented by a two-dimensional matrix. The row indices are those 
assigned to roles as described above. Each column represents a nesting depth. The path to the imp flows
across the columns, with at most a single `1` in each column representing the role through which the
path flows at that point. There is no "end marker"; the trail of `1`s simply stops. The path for the
root imp is all `0`s. 
 
## The Moves

There are two kinds of moves:

* Apply a transformation to the sept rooted at a particular location in the `Generator` clan.
  These transformations are taken from a fixed list of possible transformations. Examples might include:
    * Re-parent this Shape with a ShapeGroup.
    * Re-parent this Shape with a Rotation.
    * Replace this Dot with a Line of length 1.
    * Insert a ______ after this Shape.

* Mark the position as either a step toward the solution (such as a proposed input grid, if the problem
  is to generate an input/output grid pair) or as the complete solution.
  
## The Predictions 

At each board position, the model is asked to predict a value and a policy. There is a separate neural net
for each of those tasks.

### Predicting the Value

The value of a position is 0.0 if no correct solution is reached from that position. Else it is 1.0 discounted by
the distance to the nearest correct solution. The actual value of a position resulting from an illegal move is 
always 0.0. (So, in essence, there is a single, distinguished board position reached by every illegal move.
One might call this position "Oops".)

The value-prediction neural net is given all components of the board, but for simplicity, it is only given 
the view of the root imp.

### Predicting the Policy

The policy for a position is a probability for each potential move,
representing the likelihood of that move being on the best solution path.

The policy-prediction neural net is executed separately for every imp in the current `Generator` clan.
It is provided all components of the board, but it is only given that one imp view.

## Problems

The problem structure is, for now, taken from `waldo-generator-game.md`.




